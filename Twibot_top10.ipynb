{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
   ],
   "id": "fd469542951a1b3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)  # URLs\n",
    "    text = re.sub(r\"@\\w+\", '', text)                    # mentions\n",
    "    text = re.sub(r\"#\", '', text)                       # remove hash\n",
    "    text = re.sub(r\"RT\", '', text)                      # retweet\n",
    "    text = re.sub(r\"[^\\w\\s]\", '', text)                 # punctuation\n",
    "    return text.lower().strip()"
   ],
   "id": "6b78b206e73d12e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_top_10_recent_tweets(tweets_list):\n",
    "    \"\"\"Get 10 most recent tweets from a list\"\"\"\n",
    "    if len(tweets_list) <= 10:\n",
    "        return tweets_list\n",
    "    # Sort by created_at and get top 10\n",
    "    sorted_tweets = sorted(tweets_list, key=lambda x: x['created_at'], reverse=True)\n",
    "    return sorted_tweets[:10]"
   ],
   "id": "7c3a06278973bdb1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "from numpy.fft import rfft, rfftfreq\n",
    "\n",
    "# Funzione di feature extraction per ogni utente\n",
    "def extract_user_features(user_df):\n",
    "    times = []\n",
    "    for d in user_df:\n",
    "        times.append(d['created_at'])\n",
    "\n",
    "    times = pd.Series(pd.to_datetime(times)).sort_values()\n",
    "    hours = times.dt.hour.values\n",
    "    days = times.dt.dayofweek.values\n",
    "\n",
    "    # intervalli tra tweet (in minuti)\n",
    "    deltas = times.diff().dt.total_seconds().dropna() / 60.0\n",
    "\n",
    "    features = {}\n",
    "    features[\"n_tweets\"] = len(user_df)\n",
    "\n",
    "    # 1. media e varianza delle ore\n",
    "    features[\"mean_hour\"] = np.mean(hours) if len(hours) > 0 else np.nan\n",
    "    features[\"std_hour\"] = np.std(hours) if len(hours) > 0 else np.nan\n",
    "\n",
    "    # 2. giorno della settimana\n",
    "    features[\"mean_dayofweek\"] = np.mean(days) if len(days) > 0 else np.nan\n",
    "    features[\"std_dayofweek\"] = np.std(days) if len(days) > 0 else np.nan\n",
    "\n",
    "    # 3. weekend vs weekday\n",
    "    features[\"pct_weekend\"] = np.mean(np.isin(days, [5, 6])) if len(days) > 0 else np.nan\n",
    "\n",
    "    # 4. parte del giorno\n",
    "    features[\"pct_night\"] = np.mean((hours >= 0) & (hours < 6)) if len(hours) > 0 else np.nan\n",
    "    features[\"pct_morning\"] = np.mean((hours >= 6) & (hours < 12)) if len(hours) > 0 else np.nan\n",
    "    features[\"pct_afternoon\"] = np.mean((hours >= 12) & (hours < 18)) if len(hours) > 0 else np.nan\n",
    "    features[\"pct_evening\"] = np.mean((hours >= 18) & (hours < 24)) if len(hours) > 0 else np.nan\n",
    "\n",
    "    # 5. entropia distribuzione oraria\n",
    "    if len(hours) > 0:\n",
    "        counts_per_hour = np.bincount(hours, minlength=24)\n",
    "        probs = counts_per_hour / counts_per_hour.sum() if counts_per_hour.sum() > 0 else np.zeros(24)\n",
    "        features[\"entropy_hours\"] = entropy(probs)\n",
    "    else:\n",
    "        features[\"entropy_hours\"] = np.nan\n",
    "\n",
    "    # 6. max tweets in una stessa ora (burst su scala 1h)\n",
    "    if len(times) > 0:\n",
    "        counts = times.dt.floor(\"h\").value_counts()\n",
    "        features[\"max_tweets_per_hour\"] = counts.max() if len(counts) > 0 else 0\n",
    "    else:\n",
    "        features[\"max_tweets_per_hour\"] = np.nan\n",
    "\n",
    "    # 7. pause medie tra tweet\n",
    "    if len(deltas) > 0:\n",
    "        features[\"mean_gap_min\"] = np.mean(deltas)\n",
    "        if len(deltas) > 1:\n",
    "            features[\"std_gap_min\"] = np.std(deltas)\n",
    "            features[\"cv_gap\"] = np.std(deltas) / (np.mean(deltas) + 1e-8)\n",
    "        else:\n",
    "            features[\"std_gap_min\"] = np.nan\n",
    "            features[\"cv_gap\"] = np.nan\n",
    "    else:\n",
    "        features[\"mean_gap_min\"] = np.nan\n",
    "        features[\"std_gap_min\"] = np.nan\n",
    "        features[\"cv_gap\"] = np.nan\n",
    "\n",
    "    # 8. autocorrelazione dei gap (lag 1)\n",
    "    if len(deltas) > 1:\n",
    "        deltas_centered = deltas - deltas.mean()\n",
    "        if len(deltas_centered) > 1 and np.std(deltas_centered) > 0:\n",
    "            autocorr = np.corrcoef(deltas_centered[:-1], deltas_centered[1:])[0, 1]\n",
    "        else:\n",
    "            autocorr = np.nan\n",
    "        features[\"gap_autocorr\"] = autocorr\n",
    "    else:\n",
    "        features[\"gap_autocorr\"] = np.nan\n",
    "\n",
    "    # 9. Fourier transform sui gap (dominant frequency)\n",
    "    if len(deltas) > 5:\n",
    "        yf = np.abs(rfft(deltas - np.mean(deltas)))\n",
    "        xf = rfftfreq(len(deltas), 1)  # unitÃ  arbitraria\n",
    "        dominant_freq = xf[np.argmax(yf[1:]) + 1] if len(yf) > 1 else 0\n",
    "        features[\"dominant_gap_freq\"] = dominant_freq\n",
    "    else:\n",
    "        features[\"dominant_gap_freq\"] = np.nan\n",
    "\n",
    "    # 10. circular encoding per ore\n",
    "    if len(hours) > 0:\n",
    "        features[\"mean_hour_sin\"] = np.mean(np.sin(2 * np.pi * hours / 24))\n",
    "        features[\"mean_hour_cos\"] = np.mean(np.cos(2 * np.pi * hours / 24))\n",
    "    else:\n",
    "        features[\"mean_hour_sin\"] = np.nan\n",
    "        features[\"mean_hour_cos\"] = np.nan\n",
    "\n",
    "    # 13. burst activity in 10 minuti (massimo numero di tweet in 10min)\n",
    "    if len(times) > 0:\n",
    "        ts_series = pd.Series(1, index=times)   # valore fittizio = 1\n",
    "        rolling_counts = ts_series.rolling(\"10min\").sum()\n",
    "        features[\"max_tweets_10min\"] = rolling_counts.max()\n",
    "    else:\n",
    "        features[\"max_tweets_10min\"] = np.nan\n",
    "\n",
    "    return features"
   ],
   "id": "2426478390adaba1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load sentence transformer model\n",
    "print(\"Loading sentence transformer model...\")\n",
    "model = SentenceTransformer('arcos02/roberta-base-bne-finetuned-twitter_DANA2')"
   ],
   "id": "6aa39a67d08d606c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Array to track already processed authors\n",
    "processed_authors = set()\n",
    "\n",
    "# Process each dataset as the \"first\" dataset\n",
    "for first_dataset_idx in range(9):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Processing with dataset {first_dataset_idx} as base\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "    # Skip if this dataset's authors are already processed\n",
    "    file_path = f'Datasets/tweet_{first_dataset_idx}.json'\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File {file_path} not found, skipping...\")\n",
    "        continue\n",
    "\n",
    "    # 1. Read first dataset and create author dict\n",
    "    print(f\"Reading base dataset: tweet_{first_dataset_idx}.json\")\n",
    "    df_first = pd.read_json(file_path)\n",
    "    print(f\"Shape tweet_{first_dataset_idx}: {df_first.shape}\")\n",
    "    df_first['created_at'] = pd.to_datetime(df_first['created_at'])\n",
    "\n",
    "    # Create dict with authors from first dataset\n",
    "    author_dict = {}\n",
    "    new_authors = []\n",
    "\n",
    "    for _, row in df_first.iterrows():\n",
    "        author_id = row['author_id']\n",
    "\n",
    "        # Only process authors not already computed\n",
    "        if author_id not in processed_authors:\n",
    "            if author_id not in author_dict:\n",
    "                author_dict[author_id] = []\n",
    "                new_authors.append(author_id)\n",
    "\n",
    "            author_dict[author_id].append({\n",
    "                'text': row['text'],\n",
    "                'created_at': row['created_at']\n",
    "            })\n",
    "\n",
    "    if not new_authors:\n",
    "        print(f\"No new authors found in dataset {first_dataset_idx}, skipping...\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Found {len(new_authors)} new authors\")\n",
    "\n",
    "    # 2. Read other datasets and add tweets from existing authors\n",
    "    for other_idx in range(first_dataset_idx, 9):\n",
    "        if other_idx == first_dataset_idx:\n",
    "            continue\n",
    "\n",
    "        other_file = f'Datasets/tweet_{other_idx}.json'\n",
    "        if not os.path.exists(other_file):\n",
    "            continue\n",
    "\n",
    "        print(f\"Reading additional tweets from: tweet_{other_idx}.json\")\n",
    "        df_other = pd.read_json(other_file)\n",
    "        df_other['created_at'] = pd.to_datetime(df_other['created_at'])\n",
    "\n",
    "        # Add tweets only from authors already in our dict\n",
    "        added_count = 0\n",
    "        for _, row in df_other.iterrows():\n",
    "            author_id = row['author_id']\n",
    "            if author_id in author_dict:\n",
    "                author_dict[author_id].append({\n",
    "                    'text': row['text'],\n",
    "                    'created_at': row['created_at']\n",
    "                })\n",
    "                added_count += 1\n",
    "\n",
    "        print(f\"  Added {added_count} tweets from existing authors\")\n",
    "\n",
    "    # 3. Get top 10 recent tweets per author and compute embeddings\n",
    "    print(\"Processing tweets and computing embeddings...\")\n",
    "    user_data = []\n",
    "\n",
    "    for author_id in tqdm(new_authors, desc=\"Computing embeddings\"):\n",
    "        # Get top 10 recent tweets\n",
    "        recent_tweets = get_top_10_recent_tweets(author_dict[author_id])\n",
    "\n",
    "        # Clean texts\n",
    "        clean_texts = [clean_text(tweet['text']) for tweet in recent_tweets]\n",
    "        clean_texts = [text if text.strip() else \"empty\" for text in clean_texts]\n",
    "\n",
    "        # Compute embeddings\n",
    "        embeddings = model.encode(clean_texts)\n",
    "        mean_embedding = np.mean(embeddings, axis=0)\n",
    "\n",
    "        times = extract_user_features(author_dict[author_id])\n",
    "        times['author_id'] = author_id\n",
    "        #times['author_id'] = times[\"author_id\"].astype(int)\n",
    "        times.update({\n",
    "            'author_id': author_id,\n",
    "            #'mean_embedding': mean_embedding,\n",
    "            #'tweet_count': len(recent_tweets),\n",
    "            'total_tweets_found': len(author_dict[author_id]),\n",
    "            #'latest_tweet': max(tweet['created_at'] for tweet in recent_tweets)\n",
    "        })\n",
    "        user_data.append(times)\n",
    "\n",
    "\n",
    "\n",
    "    # 4. Save results\n",
    "    if user_data:\n",
    "        print(f\"Saving results for {len(user_data)} users...\")\n",
    "\n",
    "        # Extract embeddings matrix\n",
    "        embeddings_matrix = np.vstack([user['mean_embedding'] for user in user_data])\n",
    "\n",
    "        # Save embeddings\n",
    "        embedding_file = f'Datasets/user_embeddings_{first_dataset_idx}.npy'\n",
    "        np.save(embedding_file, embeddings_matrix)\n",
    "\n",
    "        # Create and save user mapping\n",
    "        mapping_data = []\n",
    "        for i, user in enumerate(user_data):\n",
    "            mapping_data.append(user)\n",
    "\n",
    "        mapping_df = pd.DataFrame(mapping_data)\n",
    "        mapping_file = f'Datasets/user_mapping_{first_dataset_idx}.csv'\n",
    "        mapping_df.to_csv(mapping_file, index=False)\n",
    "\n",
    "        # Add processed authors to the set\n",
    "        processed_authors.update(new_authors)\n",
    "\n",
    "        #print(f\"â Saved {embedding_file} - Shape: {embeddings_matrix.shape}\")\n",
    "        print(f\"â Saved {mapping_file} - {len(mapping_data)} users\")\n",
    "        print(f\"â Total authors processed so far: {len(processed_authors)}\")"
   ],
   "id": "e0549dc25fec7ed2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"\\nAll processing complete!\")\n",
    "print(f\"Total unique authors processed: {len(processed_authors)}\")"
   ],
   "id": "273a9715c0de4b7d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = pd.DataFrame()\n",
    "emb_list = []\n",
    "for i in range(9):\n",
    "    emb = np.load(f'Datasets/user_embeddings_{i}.npy')\n",
    "    emb_list.append(emb)\n",
    "    users = pd.read_csv(f\"Datasets/user_mapping_{i}.csv\")\n",
    "    df = pd.concat([df, users], ignore_index=True)"
   ],
   "id": "fb74d3de86a13330",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "emb_list = np.concatenate(emb_list, axis=0)\n",
    "df.shape, emb_list.shape"
   ],
   "id": "66dc2d88d6785b4d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df.to_csv('Datasets/v2_top10_embeddings_users.csv')\n",
    "np.save('Datasets/v2_top10_embeddings.npy', emb_list)"
   ],
   "id": "b8430fde74112912",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "users.head()",
   "id": "63e7e05237120ab3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df = pd.read_csv('Datasets/v2_top10_embeddings_users.csv')",
   "id": "aee7cff49f812c97",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "labels = pd.read_csv('Datasets/label.csv')\n",
    "splits = pd.read_csv('Datasets/split.csv')"
   ],
   "id": "e2ef16a0321c0495",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def add_u_prefix(author_id):\n",
    "    if str(author_id).startswith('u'):\n",
    "        author_id = int(str(author_id)[1:])\n",
    "    return 'u' + str(int(author_id))\n"
   ],
   "id": "e509a255364a4d0a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.head()",
   "id": "29de58427584bbf4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "labels.head()",
   "id": "ce1cf7cde41aa4dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "splits.head()",
   "id": "64a9469c3761cb01",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df['author_id'] = df['author_id'].apply(add_u_prefix)\n",
    "labels['id'] = labels['id'].apply(add_u_prefix)\n",
    "splits['id'] = splits['id'].apply(add_u_prefix)"
   ],
   "id": "79ca27cf427663c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.shape, labels.shape, splits.shape",
   "id": "413344654c0f0967",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "info = pd.merge(labels, splits, how='inner', on='id')\n",
    "info = info.rename(columns={'id': 'author_id'})\n",
    "info.shape"
   ],
   "id": "e6c2ffa00449af67",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "info.head()",
   "id": "e9ef13f7c994759b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df['author_id'].head()",
   "id": "e745d1c033497e5b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_merged = pd.merge(df, info, how='inner', on='author_id')\n",
    "df_merged.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "df_merged.shape"
   ],
   "id": "8a2440961dfe098b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_merged.head()",
   "id": "8f0abf5f06579fc2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_merged.to_csv('Datasets/v2_top10_embeddings_users.csv', index=False)",
   "id": "6884a2fbd2b7a8c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('Datasets/v2_top10_embeddings_users.csv')\n",
    "df.head()"
   ],
   "id": "1a936089bacb252a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.shape",
   "id": "d154e8427b94c6a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df['n_tweets'].median()",
   "id": "36e2b7a33b4e0905",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "7adc1c0a024534bc",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
